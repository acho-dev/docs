---
title: "Quickstart"
description: "Get started with Aden LLM observability in 5 minutes"
---

## Choose Your Language

<Tabs>
  <Tab title="TypeScript">
    <Steps>
      <Step title="Install the SDK">
        ```bash
        npm install aden openai
        ```
      </Step>

      <Step title="Instrument at startup">
        Add instrumentation **before** creating any LLM clients:

        ```typescript
        import { instrument, createConsoleEmitter } from "aden";
        import OpenAI from "openai";

        await instrument({
          emitMetric: createConsoleEmitter({ pretty: true }),
          sdks: { OpenAI },
        });
        ```
      </Step>

      <Step title="Use your SDK normally">
        ```typescript
        const openai = new OpenAI();

        const response = await openai.chat.completions.create({
          model: "gpt-4o",
          messages: [{ role: "user", content: "What is 2+2?" }],
        });

        console.log(response.choices[0].message.content);
        // Metrics automatically captured: latency, tokens, model, etc.
        ```
      </Step>
    </Steps>
  </Tab>

  <Tab title="Python">
    <Steps>
      <Step title="Install the SDK">
        ```bash
        pip install aden openai
        ```
      </Step>

      <Step title="Instrument at startup">
        Add instrumentation **before** creating any LLM clients:

        ```python
        from aden import instrument, MeterOptions, create_console_emitter

        instrument(MeterOptions(
            emit_metric=create_console_emitter(pretty=True),
        ))
        ```
      </Step>

      <Step title="Use your SDK normally">
        ```python
        from openai import OpenAI

        client = OpenAI()
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "What is 2+2?"}],
        )

        print(response.choices[0].message.content)
        # Metrics automatically captured: latency, tokens, model, etc.
        ```
      </Step>
    </Steps>
  </Tab>
</Tabs>

## Multi-Provider Support

Instrument multiple LLM providers with a single setup:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import { instrument, createConsoleEmitter } from "aden";
    import OpenAI from "openai";
    import Anthropic from "@anthropic-ai/sdk";
    import { GoogleGenerativeAI } from "@google/generative-ai";

    await instrument({
      emitMetric: createConsoleEmitter({ pretty: true }),
      sdks: {
        OpenAI,
        Anthropic,
        GoogleGenerativeAI,
      },
    });

    // All providers now instrumented
    const openai = new OpenAI();
    const anthropic = new Anthropic();
    const gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
    ```
  </Tab>

  <Tab title="Python">
    ```python
    from aden import instrument, MeterOptions, create_console_emitter
    from openai import OpenAI
    from anthropic import Anthropic
    import google.generativeai as genai

    instrument(MeterOptions(
        emit_metric=create_console_emitter(pretty=True),
    ))

    # All providers now instrumented
    openai_client = OpenAI()
    anthropic_client = Anthropic()
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
    ```
  </Tab>
</Tabs>

## Framework Integrations

Using an AI framework? Aden works automatically with popular frameworks:

<CardGroup cols={2}>
  <Card title="Vercel AI SDK" icon="bolt" href="/sdk/typescript/frameworks#vercel-ai-sdk">
    generateText, streamText, generateObject
  </Card>
  <Card title="LangChain" icon="link" href="/sdk/typescript/frameworks#langchainjs">
    Chains, agents, and LCEL
  </Card>
  <Card title="LlamaIndex" icon="llama" href="/sdk/typescript/frameworks#llamaindexts">
    RAG pipelines and chat engines
  </Card>
  <Card title="PydanticAI" icon="cube" href="/sdk/python/frameworks#pydanticai">
    Type-safe Python agents
  </Card>
</CardGroup>

For frameworks that use fetch directly (Vercel AI SDK, LangChain, LlamaIndex), use fetch instrumentation:

```typescript
import { instrumentFetch, createConsoleEmitter } from "aden";

await instrumentFetch({
  emitMetric: createConsoleEmitter({ pretty: true }),
});
```

## What Gets Tracked

Every LLM API call is captured with comprehensive metrics:

| Metric | Description |
|--------|-------------|
| `input_tokens` | Prompt/input tokens used |
| `output_tokens` | Completion/output tokens generated |
| `cached_tokens` | Tokens served from prompt cache |
| `latency_ms` | Request duration in milliseconds |
| `model` | Model name (e.g., `gpt-4o`, `claude-3-5-sonnet`) |
| `provider` | Provider name (`openai`, `anthropic`, `gemini`) |
| `tool_calls` | Function/tool calls made |
| `trace_id` | OpenTelemetry-compatible trace ID |

## Production Setup

For production, connect to the Aden control server for real-time cost control:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    await instrument({
      apiKey: process.env.ADEN_API_KEY,
      serverUrl: process.env.ADEN_API_URL,
      sdks: { OpenAI },

      // Track usage per user for budgets
      getContextId: () => getCurrentUserId(),

      // Handle alerts
      onAlert: (alert) => {
        console.warn(`[${alert.level}] ${alert.message}`);
      },
    });
    ```
  </Tab>

  <Tab title="Python">
    ```python
    instrument(MeterOptions(
        api_key=os.environ["ADEN_API_KEY"],
        server_url=os.environ.get("ADEN_API_URL"),

        # Track usage per user for budgets
        get_context_id=lambda: get_current_user_id(),

        # Handle alerts
        on_alert=lambda alert: print(f"[{alert.level}] {alert.message}"),
    ))
    ```
  </Tab>
</Tabs>

## Next Steps

<CardGroup cols={2}>
  <Card title="TypeScript SDK" icon="js" href="/sdk/typescript/introduction">
    Complete TypeScript documentation
  </Card>
  <Card title="Python SDK" icon="python" href="/sdk/python/introduction">
    Complete Python documentation
  </Card>
  <Card title="Cost Control" icon="shield" href="/sdk/typescript/cost-control">
    Set up budgets and control actions
  </Card>
  <Card title="Agent Tracking" icon="robot" href="/sdk/typescript/agent-tracking">
    Track multi-agent workflows
  </Card>
</CardGroup>
