---
title: "Quickstart"
description: "Instrument your first LLM call in under 5 minutes"
---

By the end of this guide, you'll see every LLM call automatically tracked with latency, tokens, and cost. This is the foundation for connecting your agents to Aden's shared intelligence network.

## Prerequisites

<CardGroup cols={2}>
  <Card title="Runtime" icon="code">
    Node.js 18+ or Python 3.9+
  </Card>
  <Card title="LLM Provider" icon="key">
    API key for OpenAI, Anthropic, or Google
  </Card>
</CardGroup>

## Step 1: Install the SDK

<Tabs>
  <Tab title="TypeScript">
    ```bash
    npm install aden openai
    ```
  </Tab>
  <Tab title="Python">
    ```bash
    pip install aden openai
    ```
  </Tab>
</Tabs>

## Step 2: Instrument Your Application

Add instrumentation **before** creating any LLM clients. This wraps the SDK to capture metrics automatically.

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import { instrument, createConsoleEmitter } from "aden";
    import OpenAI from "openai";

    // Instrument at startup - must come before creating clients
    await instrument({
      emitMetric: createConsoleEmitter({ pretty: true }),
      sdks: { OpenAI },
    });
    ```
  </Tab>
  <Tab title="Python">
    ```python
    from aden import instrument, MeterOptions, create_console_emitter

    # Instrument at startup - must come before creating clients
    instrument(MeterOptions(
        emit_metric=create_console_emitter(pretty=True),
    ))
    ```
  </Tab>
</Tabs>

## Step 3: Make Your First Call

Use your LLM SDK exactly as you normally would. Aden captures metrics transparently.

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    const openai = new OpenAI();

    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [{ role: "user", content: "What is 2+2?" }],
    });

    console.log(response.choices[0].message.content);
    ```
  </Tab>
  <Tab title="Python">
    ```python
    from openai import OpenAI

    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "What is 2+2?"}],
    )

    print(response.choices[0].message.content)
    ```
  </Tab>
</Tabs>

## Step 4: See It Working

Run your code. You'll see the analytics tab being populated and data being streamed into Aden.
Every LLM call is now tracked with latency, token usage, model, and estimated cost.

## You're Instrumented

Your agent is ready to join the shared intelligence network. In production, Aden connects your agents to shared memory - where every department's knowledge flows to every other, and your agents learn from each interaction across your organization.

## Connect to Production

<Tabs>
  <Tab title="I have credentials">
    Replace the console emitter with your Aden server connection:

    <Tabs>
      <Tab title="TypeScript">
        ```typescript
        await instrument({
          apiKey: process.env.ADEN_API_KEY,
          serverUrl: process.env.ADEN_API_URL,
          sdks: { OpenAI },

          // Track usage per user for budgets
          getContextId: () => getCurrentUserId(),

          // Handle real-time alerts
          onAlert: (alert) => {
            console.warn(`[${alert.level}] ${alert.message}`);
          },
        });
        ```
      </Tab>
      <Tab title="Python">
        ```python
        instrument(MeterOptions(
            api_key=os.environ["ADEN_API_KEY"],
            server_url=os.environ.get("ADEN_API_URL"),

            # Track usage per user for budgets
            get_context_id=lambda: get_current_user_id(),

            # Handle real-time alerts
            on_alert=lambda alert: print(f"[{alert.level}] {alert.message}"),
        ))
        ```
      </Tab>
    </Tabs>

    Your calls now stream to the Aden dashboard in real-time, with analytics populating as your agents run.
  </Tab>
  <Tab title="Get access">
    Ready to see your metrics in a live dashboard with shared intelligence across your organization?

    <Card
      title="Book a Discovery Call"
      icon="calendar"
      href="https://calendly.com/contact_aden/discovery-call"
    >
      Get your API key, dashboard access, and connect your agents to shared intelligence.
    </Card>
  </Tab>
</Tabs>

## Next Steps

<CardGroup cols={2}>
  <Card title="TypeScript SDK" icon="js" href="/sdk/typescript/introduction">
    Multi-provider support, frameworks, and advanced configuration
  </Card>
  <Card title="Python SDK" icon="python" href="/sdk/python/introduction">
    Complete Python documentation and examples
  </Card>
  <Card title="Cost Control" icon="shield" href="/sdk/typescript/cost-control">
    Set budgets, rate limits, and automatic model degradation
  </Card>
  <Card title="Agent Tracking" icon="robot" href="/sdk/typescript/agent-tracking">
    Trace multi-agent workflows with spans and named agents
  </Card>
</CardGroup>
