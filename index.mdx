---
title: "Introduction"
description: "Welcome to the Aden Documentation"
---

## LLM Observability & Cost Control

Aden provides SDKs for TypeScript and Python that automatically track every LLM API call in your application - usage, latency, costs - and give you real-time controls to prevent budget overruns.

<CardGroup cols={2}>
  <Card
    title="TypeScript SDK"
    icon="js"
    href="/sdk/typescript/introduction"
  >
    Instrument OpenAI, Anthropic, and Gemini in Node.js applications.
  </Card>
  <Card
    title="Python SDK"
    icon="python"
    href="/sdk/python/introduction"
  >
    Track LLM usage in Python with support for FastAPI, Django, and more.
  </Card>
</CardGroup>

## Key Features

<CardGroup cols={2}>
  <Card title="Multi-Provider Support" icon="layer-group">
    Works with OpenAI, Anthropic Claude, and Google Gemini with a single integration.
  </Card>
  <Card title="Zero Code Changes" icon="wand-magic-sparkles">
    Automatic instrumentation - just call `instrument()` once at startup.
  </Card>
  <Card title="Real-Time Cost Control" icon="shield-halved">
    Set budgets, throttle requests, and automatically degrade to cheaper models.
  </Card>
  <Card title="OpenTelemetry Compatible" icon="diagram-project">
    Trace IDs, span IDs, and agent stacks for multi-agent observability.
  </Card>
</CardGroup>

## Quick Start

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import { instrument, createConsoleEmitter } from "aden";
    import OpenAI from "openai";

    // Instrument at startup
    await instrument({
      emitMetric: createConsoleEmitter({ pretty: true }),
      sdks: { OpenAI },
    });

    // Use your SDK normally - metrics captured automatically
    const openai = new OpenAI();
    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [{ role: "user", content: "Hello!" }],
    });
    ```
  </Tab>
  <Tab title="Python">
    ```python
    from aden import instrument, MeterOptions, create_console_emitter
    from openai import OpenAI

    # Instrument at startup
    instrument(MeterOptions(
        emit_metric=create_console_emitter(pretty=True),
    ))

    # Use your SDK normally - metrics captured automatically
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}],
    )
    ```
  </Tab>
</Tabs>

## What Gets Tracked

Every LLM API call is captured with comprehensive metrics:

| Metric | Description |
|--------|-------------|
| `input_tokens` | Prompt/input tokens used |
| `output_tokens` | Completion/output tokens generated |
| `cached_tokens` | Tokens served from prompt cache |
| `latency_ms` | Request duration in milliseconds |
| `model` | Model name (e.g., `gpt-4o`, `claude-3-5-sonnet`) |
| `provider` | Provider name (`openai`, `anthropic`, `gemini`) |
| `tool_calls` | Function/tool calls made |
| `trace_id` | OpenTelemetry-compatible trace ID |
| `agent_stack` | Named agent context for multi-agent systems |

## Cost Control Actions

Prevent budget overruns with real-time controls:

| Action | Effect |
|--------|--------|
| **allow** | Request proceeds normally |
| **block** | Request rejected when budget exhausted |
| **throttle** | Request delayed for rate limiting |
| **degrade** | Switch to cheaper model when approaching budget |
| **alert** | Proceed with notification at warning threshold |

## Platform Products

<CardGroup cols={2}>
  <Card
    title="General Ledger API"
    icon="book"
    href="/api-reference/general-ledger/introduction"
  >
    Complete API for financial operations - chart of accounts, journal entries, AP/AR, and reporting.
  </Card>
  <Card
    title="MCP Server"
    icon="server"
    href="/mcp-server/introduction"
  >
    Model Context Protocol server for AI-powered financial operations.
  </Card>
</CardGroup>

## Need Help?

<Card
  title="Book a Discovery Call"
  icon="calendar"
  href="https://calendly.com/contact_aden/discovery-call"
>
  Schedule a call with our team to learn more.
</Card>
